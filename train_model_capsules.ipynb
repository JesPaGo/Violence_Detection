{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from Scripts.salientclassifier import SalientClassifier\n",
    "from torchsummary import summary\n",
    "from Scripts.ssi import SalientSuperImage\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "\n",
    "##################################################################################################################################################################################\n",
    "\n",
    "color_jitter = transforms.ColorJitter(random.uniform(0.1, 0.5),random.uniform(0.1, 0.5),random.uniform(0.1, 0.5),random.uniform(0.01, 0.15))\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomApply([color_jitter], p=0.5),\n",
    "    transforms.RandomAutocontrast(p=0.5),\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "###################################################################################################################################################################################\n",
    "\n",
    "train_ds =  SalientSuperImage(root_dir='/home/jparejo/projects/VD/SaliNet/dataset/SCVD/SCVD_converted_sec_split/Train', num_secs=1, k=12, sampler='uniform', aspect_ratio='480p_A', grid_shape=(4,3), transform=train_transform)\n",
    "test_ds =  SalientSuperImage(root_dir='/home/jparejo/projects/VD/SaliNet/dataset/SCVD/SCVD_converted_sec_split/Test', num_secs=1, k=12, sampler='uniform', aspect_ratio='480p_A', grid_shape=(4,3), transform=test_transform)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "test_loader = torch.utils.data.DataLoader(test_ds, batch_size=1, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "#\n",
    "import wandb\n",
    "\n",
    "def train_step(model: torch.nn.Module,\n",
    "            dataloader: torch.utils.data.DataLoader,\n",
    "            loss_fn: torch.nn,\n",
    "            optimizer: torch.optim,\n",
    "            device: torch.device) -> tuple:\n",
    "    train_loss, train_correct, train_total = 0, 0, 0\n",
    "    for n_batch, (X, y) in enumerate(tqdm(dataloader)):\n",
    "        # Train step\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        y_pred = model(X)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        loss.backward()\n",
    "        weight_before = model.fc.weight.clone()  # Suponiendo que tienes una capa fc1 en tu modelo\n",
    "        optimizer.step()\n",
    "        weight_after = model.fc.weight\n",
    "        print(\"Cambio en pesos:\", (weight_before - weight_after).abs().mean())\n",
    "        \n",
    "        # Update accuracy\n",
    "        _, predicted = torch.max(y_pred.data, 1)\n",
    "        train_correct += (predicted == y).sum().item()\n",
    "        train_total += y.size(0) # Count the number of labels           \n",
    "        train_loss += loss.item() # Accumulate loss for each batch           \n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            print(f\"Layer {name} | Gradient max: {param.grad.max()} | min: {param.grad.min()}\")\n",
    "\n",
    "        \n",
    "    # calculate average losses\n",
    "    train_loss /= len(dataloader)\n",
    "    train_acc = 100. * train_correct / train_total\n",
    "    return model, train_loss, train_acc\n",
    "\n",
    "def test_step(model: torch.nn.Module,\n",
    "            dataloader: torch.utils.data.DataLoader,\n",
    "            loss_fn: torch.nn,\n",
    "            device: torch.device,\n",
    "            retrieve_values: bool = False) -> tuple:\n",
    "\n",
    "    val_loss, val_correct, val_total = 0, 0, 0\n",
    "    y_total = []\n",
    "    y_pred_prob_total = []\n",
    "    model.to(device)\n",
    "    with torch.inference_mode():\n",
    "        for n_batch, (X, y) in enumerate(tqdm(dataloader)):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            \n",
    "            y_pred = model(X)\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            y_total.append(y)\n",
    "            y_pred_prob_total.append(torch.softmax(y_pred, dim=1))\n",
    "            \n",
    "            # Update accuracy\n",
    "            _, predicted = torch.max(y_pred.data, 1)\n",
    "            val_correct += (predicted == y).sum().item()\n",
    "            val_total += y.size(0)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "        # calculate average losses\n",
    "        val_loss /= len(dataloader)\n",
    "        val_acc = 100. * val_correct / val_total\n",
    "    if retrieve_values:\n",
    "        y_total = torch.cat(y_total, dim=0)\n",
    "        y_pred_prob_total = torch.cat(y_pred_prob_total, dim=0)\n",
    "        return val_loss, val_acc, y_total, y_pred_prob_total\n",
    "    else:\n",
    "        return val_loss, val_acc\n",
    "\n",
    "## traces is a dict which define the metrics to be stored and the app to send them to\n",
    "## traces = {'wandb': {'session': {'project': 'VD', 'group': 'SaliNet', 'name': 'SaliNet-V0'},\n",
    "#              'config': {'learning_rate': 0.001, 'architecture': 'Salinet-2m', 'dataset': \"SCVD\", 'epochs': 10, 'footnote': \"Some notes.\"},\n",
    "#                                   'name': 'SaliNet'},\n",
    "#           'metrics = ['train_met', 'test_met', 'random_ex', 'pr', 'cm']},\n",
    "#       'tensorboard': ['performance']}\n",
    "# Las métricas que guardamos serán las siguientes:\n",
    "# - train_met: accuracy + loss en el conjunto de entrenamiento\n",
    "# - test_met: accuracy + loss en el conjunto de test\n",
    "# - random_ex: ejemplos aleatorios de imágenes y sus predicciones (en prob.)\n",
    "# - pr: curva PR de ambas clases\n",
    "# - cm: matriz de confusión\n",
    "# - performance: tiempos de ejecución de cada componente del modelo\n",
    "## Hay que preinicializar wandb\n",
    "                \n",
    "def train(model: torch.nn.Module,\n",
    "        train_dataloader: torch.utils.data.DataLoader,\n",
    "        test_dataloader: torch.utils.data.DataLoader,\n",
    "        save_path: str,\n",
    "        device: torch.device,\n",
    "        loss_fn: torch.nn,\n",
    "        optimizer: torch.optim,\n",
    "        lr_scheduler: torch.optim.lr_scheduler = None,\n",
    "        epochs: int = 100,\n",
    "        traces: dict = {},\n",
    "        verbose: bool = True):\n",
    "    \n",
    "    ## Initialization \n",
    "    best_acc = 0\n",
    "    model.to(device)\n",
    "    traces_wandb, traces_tensorboard = traces.get('wandb', []), traces.get('tensorboard', [])\n",
    "    \n",
    "    # ## Traces\n",
    "    # prof = torch.profiler.profile(\n",
    "    #     schedule = traces_tensorboard['schedule'],\n",
    "    #     on_trace_ready = torch.profiler.tensorboard_trace_handler(traces_tensorboard['save_path']),\n",
    "    #     record_shapes=True,\n",
    "    #     with_stack=False)\n",
    "    \n",
    "    # if traces_wandb is not None:\n",
    "    #     wandb.init(project=traces_wandb['session']['project'],\n",
    "    #             group=traces_wandb['session']['group'],\n",
    "    #             name=traces_wandb['session']['name'],\n",
    "    #             config=traces_wandb['session']['config'])\n",
    "    \n",
    "    # prof.start()\n",
    "    for epoch in range(1, epochs+1):\n",
    "        last_epoch = epoch == epochs\n",
    "        # prof.step()\n",
    "        ## Train\n",
    "        model.train()\n",
    "        model, train_loss, train_acc = train_step(model, train_dataloader, loss_fn, optimizer, device)\n",
    "\n",
    "        # Update learning rate\n",
    "        if lr_scheduler is not None:\n",
    "            if verbose:\n",
    "                print(\"Updating learning rate.\")\n",
    "            lr_scheduler.step()\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"Learning Rate: {current_lr}\")\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                print(f\"Layer {name} | Gradient max: {param.grad.max()} | min: {param.grad.min()}\")\n",
    "        \n",
    "        ## Test\n",
    "        model.eval()\n",
    "        val_loss, val_acc, y, y_prob = test_step(model, test_dataloader, loss_fn, device, retrieve_values=True)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_model = model.state_dict()\n",
    "            torch.save(best_model, save_path)\n",
    "\n",
    "        # # Save traces to wandb\n",
    "        # if wandb.run is not None:\n",
    "        #     if 'train_met' in traces_wandb['metrics']:\n",
    "        #         wandb.log({'train_loss': train_loss, 'train_acc': train_acc}, step=epoch)\n",
    "        #     if 'test_met' in traces_wandb['metrics']:\n",
    "        #         wandb.log({'val_loss': val_loss, 'val_acc': val_acc}, step=epoch)\n",
    "        #     if 'random_ex' in traces_wandb['metrics']:\n",
    "        #         pass\n",
    "        #     if 'pr' in traces_wandb['metrics'] and last_epoch: # Only save PR curve in the last epoch\n",
    "        #         wandb.log({'roc': wandb.plot.roc_curve(y.cpu(),\n",
    "        #             y_prob.cpu(), labels=[\"Normal\", \"Violence\"])}, step=epoch)\n",
    "        #     if 'cm' in traces_wandb['metrics'] and last_epoch: # Only save confusion matrix in the last epoch\n",
    "        #         wandb.log({'confusion_matrix': wandb.plot.confusion_matrix(\n",
    "        #             y_prob.cpu(), y.cpu().tolist(), class_names=[\"Normal\", \"Violence\"])}, step=epoch)\n",
    "        #     if verbose and last_epoch:\n",
    "        #         print(f\"Epoch {epoch}/{epochs}: Metrics sent to wandb.\")\n",
    "        # else:\n",
    "        #     print(\"No wandb run detected. Skipping logging to wandb.\")\n",
    "        \n",
    "        # # Save traces to tensorboard\n",
    "        # if 'performance' in traces_tensorboard:\n",
    "        #     pass\n",
    "            \n",
    "        if verbose:\n",
    "            print(f\"Epoch {epoch}/{epochs}: Train Loss: {train_loss:.4f} || Train Acc: {train_acc:.2f}% || Val Loss: {val_loss:.4f} || Val Acc: {val_acc:.2f}%\")\n",
    "    # wandb.finish()\n",
    "    # prof.stop()\n",
    "    print(\"Finished Training.\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight\n",
      "conv1.kernel_fn.cp\n",
      "bn1.weight\n",
      "bn1.bias\n",
      "layer1.0.conv1.weight\n",
      "layer1.0.bn1.weight\n",
      "layer1.0.bn1.bias\n",
      "layer2.0.conv1.weight\n",
      "layer2.0.bn1.weight\n",
      "layer2.0.bn1.bias\n",
      "layer2.0.downsample.0.weight\n",
      "layer2.0.downsample.1.weight\n",
      "layer2.0.downsample.1.bias\n",
      "layer3.0.conv1.weight\n",
      "layer3.0.bn1.weight\n",
      "layer3.0.bn1.bias\n",
      "layer3.0.downsample.0.weight\n",
      "layer3.0.downsample.1.weight\n",
      "layer3.0.downsample.1.bias\n",
      "layer4.0.conv1.weight\n",
      "layer4.0.bn1.weight\n",
      "layer4.0.bn1.bias\n",
      "layer4.0.downsample.0.weight\n",
      "layer4.0.downsample.1.weight\n",
      "layer4.0.downsample.1.bias\n",
      "fc.weight\n",
      "fc.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.model.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "def get_model(kind: str, verbose: bool = True) -> torch.nn.Module:\n",
    "    match kind:\n",
    "        case 'paper':\n",
    "            model = SalientClassifier(\"salinet2m\", num_classes=3)\n",
    "            checkpoint = torch.load(\"weights/SalientClassifier-Salinet2m-SCVD.pth\")\n",
    "            model.load_state_dict(checkpoint)\n",
    "            for param in model.model.parameters():\n",
    "                param.requires_grad = False\n",
    "            num_classes = 2\n",
    "            model.model.fc = nn.Linear(model.model.fc.in_features, num_classes)\n",
    "        case 'scratch':\n",
    "            model = SalientClassifier(\"salinet2m\", num_classes=2)\n",
    "        case 'keep_training':\n",
    "            model = SalientClassifier(\"salinet2m\", num_classes=2)\n",
    "            checkpoint = torch.load(\"weights/SalientClassifier-caps.pth\")\n",
    "            model.load_state_dict(checkpoint)\n",
    "            for name, param in model.model.named_parameters():\n",
    "                if (\"fc\" not in name):\n",
    "                    param.requires_grad = False\n",
    "        case _:\n",
    "            raise ValueError(\"Invalid model kind.\")\n",
    "    if verbose:\n",
    "        summary(model, input_size = (4, 3, 672, 672), verbose=0, col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"], col_width=20)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_835125/2136199232.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\"weights/SalientClassifier-Salinet2m-SCVD.pth\")\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Define loss functions\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "model = get_model('paper')\n",
    "\n",
    "# Define optimizer and learning rate scheduler\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-3, momentum=0.9)\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "num_epochs = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/58 [00:23<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'SalientClassifier' object has no attribute 'fc1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./weights/SalientClassifier-caps.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m### <-\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mce_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraces\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 127\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_dataloader, test_dataloader, save_path, device, loss_fn, optimizer, lr_scheduler, epochs, traces, verbose)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# prof.step()\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m## Train\u001b[39;00m\n\u001b[1;32m    126\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m--> 127\u001b[0m model, train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m# Update learning rate\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lr_scheduler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[13], line 20\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(model, dataloader, loss_fn, optimizer, device)\u001b[0m\n\u001b[1;32m     18\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(y_pred, y)\n\u001b[1;32m     19\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 20\u001b[0m weight_before \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mclone()  \u001b[38;5;66;03m# Suponiendo que tienes una capa fc1 en tu modelo\u001b[39;00m\n\u001b[1;32m     21\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     22\u001b[0m weight_after \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfc1\u001b[38;5;241m.\u001b[39mweight\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1931\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1929\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1930\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1931\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1932\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1933\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SalientClassifier' object has no attribute 'fc1'"
     ]
    }
   ],
   "source": [
    "model = train(model,\n",
    "    train_dataloader=train_loader,\n",
    "    test_dataloader=test_loader,\n",
    "    save_path = \"./weights/SalientClassifier-caps.pth\", ### <-\n",
    "    loss_fn=ce_loss,\n",
    "    optimizer=optimizer,\n",
    "    lr_scheduler=lr_scheduler,\n",
    "    epochs=num_epochs,\n",
    "    device=device,\n",
    "    traces = {},\n",
    "    verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssivd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
